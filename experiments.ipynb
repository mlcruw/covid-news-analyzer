{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 8] nodename nor\n",
      "[nltk_data]     servname provided, or not known>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from training.argparser import argparser\n",
    "from training.config import Config\n",
    "from training.trainer import Trainer\n",
    "import json\n",
    "# from data.preprocessing import preprocessor_fn\n",
    "\n",
    "from data.stanford_sentiment import StanfordSentimentDataset\n",
    "from data.news_category import NewsCategoryDataset\n",
    "from data.fake_news import FakeNewsDataset\n",
    "from data.emotion_affect import EmotionAffectDataset\n",
    "\n",
    "dataset_map = {\n",
    "  'stan_sent': StanfordSentimentDataset,\n",
    "  'news_cat': NewsCategoryDataset,\n",
    "  'fake_news': FakeNewsDataset,\n",
    "  'emo_aff': EmotionAffectDataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Emotion Affect Dataset\n",
      "Reading data...\n",
      "1207\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset = EmotionAffectDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fake News Dataset\n",
      "\n",
      "Downloading data to data/datasets/fake_news_dataset using the command:\n",
      "   kaggle competitions download -c fake-news\n",
      "Not downloading. Data already downloaded\n",
      "\n",
      "Reading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset = FakeNewsDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Stanford Sentiment Analysis Dataset\n",
      "\n",
      "Downloading data to data/datasets/stanford_sentiment using the command:\n",
      "   kaggle competitions download -c sentiment-analysis-on-movie-reviews\n",
      "Not downloading. Data already downloaded\n",
      "\n",
      "Reading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset = StanfordSentimentDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.train_data['Sentiment'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on News Category Dataset\n",
      "\n",
      "Downloading data to data/datasets/news_category_dataset using the command:\n",
      "   kaggle datasets download rmisra/news-category-dataset\n",
      "Not downloading. Data already downloaded\n",
      "\n",
      "Reading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset = NewsCategoryDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "dataset.split_data(dataset_ratio=1.0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 242)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(dataset.data), \n",
    "len(dataset.train_data), len(dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91729094\n"
     ]
    }
   ],
   "source": [
    "# total = 0\n",
    "# for idx in range(dataset.data['X'].values.shape[0]):\n",
    "#     total += len(dataset.data['X'].values[idx])\n",
    "# print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.preprocessing import LemmaTokenizer, StemTokenizer\n",
    "\n",
    "# tokenizer = LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.733 seconds\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# out = tokenizer(' '.join(['styles' for i in range(1000000)]))\n",
    "# print('{:.3f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(965, 589)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "vect = CountVectorizer(min_df=5, max_features=5000)\n",
    "X_train = vect.fit_transform(dataset.train_data['X'])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = PCA(n_components=1000).fit_transform(X_train.todense())\n",
    "# print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(dataset=dataset, models=['mnb', 'svm', 'lr', 'xgb', 'ada', 'rf'], transforms=['bow', 'tfidf', 'ngram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer2 = Trainer(dataset, models=['lr'], transforms=['bow'])\n",
    "\n",
    "# trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.transformed['bow']['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = trainer.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((965,), (965,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with bow transformation\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with bow transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with tfidf transformation\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with tfidf transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with ngram transformation\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training mnb with ngram transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mnb with bow transformation\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training mnb with tfidf transformation\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training mnb with ngram transformation\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training svm with bow transformation\n",
      "\u001b[92m05-16 20:37:23\u001b[0m Training svm with bow transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training svm with bow transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    0.4s finished\n",
      "\u001b[92m05-16 20:37:24\u001b[0m Training svm with tfidf transformation\n",
      "\u001b[92m05-16 20:37:24\u001b[0m Training svm with tfidf transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\n",
      "Training svm with tfidf transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    0.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    0.4s finished\n",
      "\u001b[92m05-16 20:37:24\u001b[0m Training svm with ngram transformation\n",
      "\u001b[92m05-16 20:37:24\u001b[0m Training svm with ngram transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\n",
      "Training svm with ngram transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    0.6s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    0.7s finished\n",
      "\u001b[92m05-16 20:37:25\u001b[0m Training lr with bow transformation\n",
      "\u001b[92m05-16 20:37:25\u001b[0m Training lr with bow transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\n",
      "Training lr with bow transformation\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "\u001b[92m05-16 20:37:25\u001b[0m Training lr with tfidf transformation\n",
      "\u001b[92m05-16 20:37:25\u001b[0m Training lr with tfidf transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    0.1s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    0.1s remaining:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training lr with tfidf transformation\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.2s finished\n",
      "\u001b[92m05-16 20:37:26\u001b[0m Training lr with ngram transformation\n",
      "\u001b[92m05-16 20:37:26\u001b[0m Training lr with ngram transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training lr with ngram transformation\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:    0.1s remaining:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  20 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  20 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.3s finished\n",
      "\u001b[92m05-16 20:37:26\u001b[0m Training xgb with bow transformation\n",
      "\u001b[92m05-16 20:37:26\u001b[0m Training xgb with bow transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training xgb with bow transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    6.5s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:37:35\u001b[0m Training xgb with tfidf transformation\n",
      "\u001b[92m05-16 20:37:35\u001b[0m Training xgb with tfidf transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   1.5s\n",
      "Training xgb with tfidf transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    9.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   10.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:37:47\u001b[0m Training xgb with ngram transformation\n",
      "\u001b[92m05-16 20:37:47\u001b[0m Training xgb with ngram transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   1.7s\n",
      "Training xgb with ngram transformation\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  60 | elapsed:    7.7s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:37:57\u001b[0m Training ada with bow transformation\n",
      "\u001b[92m05-16 20:37:57\u001b[0m Training ada with bow transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   1.5s\n",
      "Training ada with bow transformation\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:    0.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.4s finished\n",
      "\u001b[92m05-16 20:37:57\u001b[0m Training ada with tfidf transformation\n",
      "\u001b[92m05-16 20:37:57\u001b[0m Training ada with tfidf transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training ada with tfidf transformation\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:    0.1s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.3s finished\n",
      "\u001b[92m05-16 20:37:58\u001b[0m Training ada with ngram transformation\n",
      "\u001b[92m05-16 20:37:58\u001b[0m Training ada with ngram transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.1s\n",
      "Training ada with ngram transformation\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:    0.2s remaining:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.4s finished\n",
      "\u001b[92m05-16 20:37:58\u001b[0m Training rf with bow transformation\n",
      "\u001b[92m05-16 20:37:58\u001b[0m Training rf with bow transformation\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.0s\n",
      "Training rf with bow transformation\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:38:02\u001b[0m Training rf with tfidf transformation\n",
      "\u001b[92m05-16 20:38:02\u001b[0m Training rf with tfidf transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.2s\n",
      "Training rf with tfidf transformation\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:38:08\u001b[0m Training rf with ngram transformation\n",
      "\u001b[92m05-16 20:38:08\u001b[0m Training rf with ngram transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.3s\n",
      "Training rf with ngram transformation\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    5.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .......... (step 1 of 2) Processing tranform, total=   0.1s\n",
      "[Pipeline] ............. (step 2 of 2) Processing model, total=   0.2s\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>transform</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mnb</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.801036</td>\n",
       "      <td>0.801036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mnb</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.731606</td>\n",
       "      <td>0.731606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnb</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.801036</td>\n",
       "      <td>0.801036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svm</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.982383</td>\n",
       "      <td>0.982383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>svm</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.941969</td>\n",
       "      <td>0.941969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>svm</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.864249</td>\n",
       "      <td>0.864249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lr</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.906736</td>\n",
       "      <td>0.906736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lr</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.801036</td>\n",
       "      <td>0.801036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lr</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.905699</td>\n",
       "      <td>0.905699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xgb</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.772021</td>\n",
       "      <td>0.772021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>xgb</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.846632</td>\n",
       "      <td>0.846632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xgb</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.770984</td>\n",
       "      <td>0.770984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ada</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.541969</td>\n",
       "      <td>0.541969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ada</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.608290</td>\n",
       "      <td>0.608290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ada</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.541969</td>\n",
       "      <td>0.541969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.832124</td>\n",
       "      <td>0.832124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rf</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>0.969948</td>\n",
       "      <td>0.969948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rf</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.823834</td>\n",
       "      <td>0.823834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model transform  precision  accuracy\n",
       "0    mnb       bow   0.801036  0.801036\n",
       "1    mnb     tfidf   0.731606  0.731606\n",
       "2    mnb     ngram   0.801036  0.801036\n",
       "3    svm       bow   0.982383  0.982383\n",
       "4    svm     tfidf   0.941969  0.941969\n",
       "5    svm     ngram   0.864249  0.864249\n",
       "6     lr       bow   0.906736  0.906736\n",
       "7     lr     tfidf   0.801036  0.801036\n",
       "8     lr     ngram   0.905699  0.905699\n",
       "9    xgb       bow   0.772021  0.772021\n",
       "10   xgb     tfidf   0.846632  0.846632\n",
       "11   xgb     ngram   0.770984  0.770984\n",
       "12   ada       bow   0.541969  0.541969\n",
       "13   ada     tfidf   0.608290  0.608290\n",
       "14   ada     ngram   0.541969  0.541969\n",
       "15    rf       bow   0.832124  0.832124\n",
       "16    rf     tfidf   0.969948  0.969948\n",
       "17    rf     ngram   0.823834  0.823834"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 10, 'model__kernel': 'rbf'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.gridsearch['svm']['bow'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model precision is 0.98 ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model precision is 0.98 ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model is svm ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model is svm ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best feature transformation is bow ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best feature transformation is bow ']\n",
      "\u001b[92m05-16 20:38:43\u001b[0m [\"The best parameters found via GridSearch are {'model__C': 10, 'model__kernel': 'rbf'}\"]\n",
      "\u001b[92m05-16 20:38:43\u001b[0m [\"The best parameters found via GridSearch are {'model__C': 10, 'model__kernel': 'rbf'}\"]\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model configuration is ', \"dataset: news_cat, model: svm, feats: bow, save_path: bow_0, continue_train: False, load_path: None, test: None, params: {'model__C': 10, 'model__kernel': 'rbf'}\"]\n",
      "\u001b[92m05-16 20:38:43\u001b[0m ['The best model configuration is ', \"dataset: news_cat, model: svm, feats: bow, save_path: bow_0, continue_train: False, load_path: None, test: None, params: {'model__C': 10, 'model__kernel': 'rbf'}\"]\n",
      "\u001b[92m05-16 20:38:43\u001b[0m    model transform  precision  accuracy\n",
      "0    mnb       bow   0.801036  0.801036\n",
      "1    mnb     tfidf   0.731606  0.731606\n",
      "2    mnb     ngram   0.801036  0.801036\n",
      "3    svm       bow   0.982383  0.982383\n",
      "4    svm     tfidf   0.941969  0.941969\n",
      "5    svm     ngram   0.864249  0.864249\n",
      "6     lr       bow   0.906736  0.906736\n",
      "7     lr     tfidf   0.801036  0.801036\n",
      "8     lr     ngram   0.905699  0.905699\n",
      "9    xgb       bow   0.772021  0.772021\n",
      "10   xgb     tfidf   0.846632  0.846632\n",
      "11   xgb     ngram   0.770984  0.770984\n",
      "12   ada       bow   0.541969  0.541969\n",
      "13   ada     tfidf   0.608290  0.608290\n",
      "14   ada     ngram   0.541969  0.541969\n",
      "15    rf       bow   0.832124  0.832124\n",
      "16    rf     tfidf   0.969948  0.969948\n",
      "17    rf     ngram   0.823834  0.823834\n",
      "\u001b[92m05-16 20:38:43\u001b[0m    model transform  precision  accuracy\n",
      "0    mnb       bow   0.801036  0.801036\n",
      "1    mnb     tfidf   0.731606  0.731606\n",
      "2    mnb     ngram   0.801036  0.801036\n",
      "3    svm       bow   0.982383  0.982383\n",
      "4    svm     tfidf   0.941969  0.941969\n",
      "5    svm     ngram   0.864249  0.864249\n",
      "6     lr       bow   0.906736  0.906736\n",
      "7     lr     tfidf   0.801036  0.801036\n",
      "8     lr     ngram   0.905699  0.905699\n",
      "9    xgb       bow   0.772021  0.772021\n",
      "10   xgb     tfidf   0.846632  0.846632\n",
      "11   xgb     ngram   0.770984  0.770984\n",
      "12   ada       bow   0.541969  0.541969\n",
      "13   ada     tfidf   0.608290  0.608290\n",
      "14   ada     ngram   0.541969  0.541969\n",
      "15    rf       bow   0.832124  0.832124\n",
      "16    rf     tfidf   0.969948  0.969948\n",
      "17    rf     ngram   0.823834  0.823834\n",
      "\u001b[92m05-16 20:38:43\u001b[0m Saving configuration:\n",
      "\u001b[92m05-16 20:38:43\u001b[0m Saving configuration:\n",
      "\u001b[92m05-16 20:38:43\u001b[0m dataset: news_cat, model: svm, feats: bow, save_path: bow_0, continue_train: False, load_path: None, test: None, params: {'model__C': 10, 'model__kernel': 'rbf'}\n",
      "\u001b[92m05-16 20:38:43\u001b[0m dataset: news_cat, model: svm, feats: bow, save_path: bow_0, continue_train: False, load_path: None, test: None, params: {'model__C': 10, 'model__kernel': 'rbf'}\n",
      "\u001b[92m05-16 20:38:43\u001b[0m Saving done\n",
      "\u001b[92m05-16 20:38:43\u001b[0m Saving done\n"
     ]
    }
   ],
   "source": [
    "trainer.save_best(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | model   | transform   |   precision |   accuracy |\n",
      "|---:|:--------|:------------|------------:|-----------:|\n",
      "|  0 | mnb     | bow         |    0.801036 |   0.801036 |\n",
      "|  1 | mnb     | tfidf       |    0.731606 |   0.731606 |\n",
      "|  2 | mnb     | ngram       |    0.801036 |   0.801036 |\n",
      "|  3 | svm     | bow         |    0.982383 |   0.982383 |\n",
      "|  4 | svm     | tfidf       |    0.941969 |   0.941969 |\n",
      "|  5 | svm     | ngram       |    0.864249 |   0.864249 |\n",
      "|  6 | lr      | bow         |    0.906736 |   0.906736 |\n",
      "|  7 | lr      | tfidf       |    0.801036 |   0.801036 |\n",
      "|  8 | lr      | ngram       |    0.905699 |   0.905699 |\n",
      "|  9 | xgb     | bow         |    0.772021 |   0.772021 |\n",
      "| 10 | xgb     | tfidf       |    0.846632 |   0.846632 |\n",
      "| 11 | xgb     | ngram       |    0.770984 |   0.770984 |\n",
      "| 12 | ada     | bow         |    0.541969 |   0.541969 |\n",
      "| 13 | ada     | tfidf       |    0.60829  |   0.60829  |\n",
      "| 14 | ada     | ngram       |    0.541969 |   0.541969 |\n",
      "| 15 | rf      | bow         |    0.832124 |   0.832124 |\n",
      "| 16 | rf      | tfidf       |    0.969948 |   0.969948 |\n",
      "| 17 | rf      | ngram       |    0.823834 |   0.823834 |\n"
     ]
    }
   ],
   "source": [
    "print(df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllrrr}\n",
      "\\toprule\n",
      "{} & model & transform &  precision &    recall &  f1-score \\\\\n",
      "\\midrule\n",
      "0 &   mnb &       bow &   0.804145 &  0.804145 &  0.804145 \\\\\n",
      "1 &   svm &       bow &   0.872539 &  0.872539 &  0.872539 \\\\\n",
      "2 &    lr &       bow &   0.912953 &  0.912953 &  0.912953 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = trainer.pipelines['lr']['bow'].predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "with open('test.pkl', 'wb') as fw:\n",
    "    pickle.dump(best_pipeline, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('test.pkl', 'rb') as f:\n",
    "    loaded_pipeline = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=1e-06, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=20, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='elasticnet', power_t=0.5,\n",
       "                               random_state=None, shuffle=True, tol=0.001,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "params = {'stop_words':'english', 'min_df':5}\n",
    "\n",
    "vect = CountVectorizer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('covid19_articles/covid_19_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 2), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=Non...\n",
       "                 SGDClassifier(alpha=1e-06, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='hinge',\n",
       "                               max_iter=20, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='elasticnet', power_t=0.5,\n",
       "                               random_state=None, shuffle=True, tol=0.001,\n",
       "                               validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_pipeline.predict(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Angry-Disgusted\n",
       "1     Angry-Disgusted\n",
       "2               HAPPY\n",
       "3               HAPPY\n",
       "4               HAPPY\n",
       "5           Surprised\n",
       "6           Surprised\n",
       "7             Fearful\n",
       "8                 Sad\n",
       "9             Fearful\n",
       "10    Angry-Disgusted\n",
       "11          Surprised\n",
       "12          Surprised\n",
       "13                Sad\n",
       "14            Fearful\n",
       "15          Surprised\n",
       "16            Fearful\n",
       "17          Surprised\n",
       "18          Surprised\n",
       "19            Fearful\n",
       "Name: Emotion 1-7, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Emotion 1-7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "article0 = data['text'][0]\n",
    "\n",
    "def eval_emotion(article_text):\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    pred_emotions = loaded_pipeline.predict(sentences)\n",
    "    final_emotion = stats.mode(pred_emotions).mode[0]\n",
    "    return final_emotion\n",
    "\n",
    "print(eval_emotion(article0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(article_text):\n",
    "    # load all 4 models\n",
    "    # run predictions on all 4 models\n",
    "    # return a dict {\"emotion\": pred_emotion, \"category\": pred_category, \"fake\": pred_fake, \"sentiment\": pred_sentiment}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emo.model           emo_aff.csv         emo_aff_clean.csv   emo_aff_results.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/model_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('output/model_dump/emo_aff_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>transform</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.906736</td>\n",
       "      <td>0.906736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.912953</td>\n",
       "      <td>0.912953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linearsvm</td>\n",
       "      <td>bow</td>\n",
       "      <td>0.960622</td>\n",
       "      <td>0.960622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linearsvm</td>\n",
       "      <td>ngram</td>\n",
       "      <td>0.962694</td>\n",
       "      <td>0.962694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model transform  precision  accuracy\n",
       "0         lr       bow   0.906736  0.906736\n",
       "1         lr     ngram   0.912953  0.912953\n",
       "2  linearsvm       bow   0.960622  0.960622\n",
       "3  linearsvm     ngram   0.962694  0.962694"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_csv('output/model_dump/emo_aff_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it is very unpleasant i am afraid of the polic...</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pickle nearly had a fit he barked and he barke...</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he shut the door in nutkins face</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>old mr brown turned up his eye in disgust at t...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and to this day if you meet nutkin up a tree a...</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>ah said the father what fear we have had for you</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>yes father answered he i have travelled all ov...</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>well said they you are come back and we will n...</td>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>then they hugged and kissed their dear little ...</td>\n",
       "      <td>2</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>so master thumb stayed at home with his father...</td>\n",
       "      <td>2</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1207 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      X  y  index\n",
       "0     it is very unpleasant i am afraid of the polic...  1     35\n",
       "1     pickle nearly had a fit he barked and he barke...  0     46\n",
       "2                      he shut the door in nutkins face  0     24\n",
       "3     old mr brown turned up his eye in disgust at t...  0     51\n",
       "4     and to this day if you meet nutkin up a tree a...  0     77\n",
       "...                                                 ... ..    ...\n",
       "1202   ah said the father what fear we have had for you  2     98\n",
       "1203  yes father answered he i have travelled all ov...  2     99\n",
       "1204  well said they you are come back and we will n...  2    102\n",
       "1205  then they hugged and kissed their dear little ...  2    103\n",
       "1206  so master thumb stayed at home with his father...  2    104\n",
       "\n",
       "[1207 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37464bitmlcondaa8f3f69cf251458aa86fbbc582b758f6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
